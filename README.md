# Assignment-2
Data Bias in Toxic Wikipedia Comments
Perspective's current model is a multi-headed model capable of detecting different types of of toxicity such as threats, obscenity, insults, and identity-based hate.

When I was initally exploring the scord dataset, I disagreed with a few of the comments' labels. For example, many of the comments labeled 'insult', I thought should also be considered 'identity hate'. However, they were not. It was interesting to see how different individuals have different ideas on how comments should be labeled because I found that many of the labels overlap each other. Therefore, before designing or performing any tests I created a representation of each category against their calculated counts in order to get a better understanding of the different categories of toxicity: toxic, severe toxic, obsecene, threat, insult, and identity_hate.

I also noticed a few trends in the data. For example, most comments irrespective of category were under 100 characters. No comments exceeded 900 characters. After cleaning the data, I found that the words with the highest frequency were 'fuck','article','page','wikipedia',and 'talk'. In order to perform this frequency test, I removed punctuation, line breaks, links, symbols, special characters, and made all the comments lower case. I also removed stop words which don't provide useful information in order to give more importance to other imformation.

Initially, I questioned what differentiated a toxic comment from a severe toxic one. Do multiple other labels on comments classify it as severe toxic? Therefore, my hypothesis is: a comment is likely to be tagged severe toxic if it has more than one other tagged category. I used a train-test split model in order to estimate the performance of perspectives algorithm. I also checked the size of the train and test data before finding the correlation of different labels against eachother using a cross correlation matrix. I found that not all comments with 'obscene' and 'insult' labels were considered to be severe toxic. Similarly, I found that not all comments with 'threat' and 'identity hate' labels were considered to be severe toxic. My hypothesis that a comment is more likely to be tagged 'severe toxic' if it has more than one other tagged category was proven to be false.

I believe these results are due to the fact that my interpretations of the labels are different to the algorithms interpretation. Using logistic regression would have given me a better understanding of the data. Moreover, low scores may indicate that the model is unable to distinguish between non toxic and toxic comments.
