# Assignment-2
Data Bias in Toxic Wikipedia Comments
Perspective's current model is a multi-headed model capable of detecting different types of of toxicity such as threats, obscenity, insults, and identity-based hate.

When I was initally exploring the scord dataset, I disagreed with a few of the comments' labels. For example, many of the comments labeled insult, I thought should also be considered identity hate. However, they were not. It was interesting to see how different individuals have different ideas on how comments should be labeled because I found many of the labels to overlap eachother. Therefore, before designing or performing any tests I created a representation of each category against their calculated counts in order to get a better understanding of the different categories of toxicity: toxic, severe toxic, obsecene, threat, insult, and identity_hate.

I also noticed a few trends in the data. For example, most comments irrespective of category were between 0 and 100 characters. No comments exceeded 900 characters. After cleaning the data, I found that the words with the highest frequency were 'fuck','article','page','wikipedia',and 'talk'. In order to perform this frequency test, I emoved punctuation, line breaks, links, symbols, special characters, and made all the comments lower case. I also removed stop words which don't proivide useful information in order to give more importance to other imformation.

Initially, I questioned what made a comment go from being toxic to severe toxic? Does multiple tags on comments make it severe toxic? Therefore, my hypothesis is: a comment is likely to be tagged severe toxic if it has more than one other tagged category. I used a train-test split model in order to estimate the performance of perspectives algorithm. I also checked the size of the train and test data before finding out the correlation of different labels against eachother using a cross correlation matrix. I found that not all comments with 'obscene' and 'insult' labels were considered to be severe toxic. Similarly, I found that not all comments with 'obscene' and 'insult' labels were considered to be severe toxic. My hypothesis that a comment is more likely to be tagged 'severe toxic' if it has more than one other tagged category was proven to be false.

I believe these results are due to the fact that I have different interpretations of the labels opposed to the algorithms interpretation. Moreover, using logistic Regression would have given me a better understanding Moreover, low scores may indicate that the model is unable to distinguish between non toxic and toxic comments.
